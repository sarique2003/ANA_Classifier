{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(os.listdir('/content/drive/MyDrive/6th_sem_immune_dataset/SubClassification'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIuwnHlxlAM8",
        "outputId": "53f9b3ec-002b-4cc3-8049-46b2e188d8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Nuclear', 'Cytoplasmic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('/content/drive/MyDrive/6th_sem_immune_dataset/SubClassification/Nuclear/Pleomorphic'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvjVwIv3kTBG",
        "outputId": "86b0aa2e-f902-4111-f50d-55385651e610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AC-14', 'AC-13', 'train.pickle', 'test.pickle', 'weights']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aynUUC0kgHAM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "import pickle as pkl\n",
        "import cv2\n",
        "import h5py\n",
        "\n",
        "#from preprocess_data import dataloader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import rotate, AffineTransform, warp, rescale\n",
        "from skimage.util import random_noise\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Lambda, Input, Flatten, Dense, Concatenate, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Input, Sequential, Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Lambda, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    Class for loading data directly from image files in directories.\n",
        "    \"\"\"\n",
        "    def __init__(self, width, height, cells, data_path, output_path):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.cells = cells  # Number of color channels (typically 3 for RGB)\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def _open_image(self, path):\n",
        "        \"\"\"\n",
        "        Open an image and convert it into a numpy array.\n",
        "        \"\"\"\n",
        "        image = Image.open(path).convert('L')  # Convert image to grayscale\n",
        "        image = image.resize((self.width, self.height))\n",
        "        data = np.asarray(image)\n",
        "        data = np.array(data, dtype='float64')\n",
        "        return np.expand_dims(data, axis=-1)\n",
        "\n",
        "    def _rotate_image(self, image, angle):\n",
        "        \"\"\"\n",
        "        Rotate an image by the specified angle and return the rotated image as a numpy array.\n",
        "        \"\"\"\n",
        "        rotated_image = image.rotate(angle)\n",
        "        data = np.asarray(rotated_image)\n",
        "        data = np.array(data, dtype='float64')\n",
        "        return np.expand_dims(data, axis=-1)\n",
        "\n",
        "    def load_images_from_subfolders(self, folder_name):\n",
        "     image_paths = {}\n",
        "     class_labels = {}\n",
        "     label = 0\n",
        "\n",
        "    # Iterate through subfolders\n",
        "     for subfolder in os.scandir(folder_name):\n",
        "        # Only process directories that are not named \"weights\"\n",
        "        if subfolder.is_dir() and subfolder.name.lower() not in [\"weights\"]:\n",
        "            subfolder_name = subfolder.name\n",
        "            class_labels[subfolder_name] = label\n",
        "            label += 1\n",
        "\n",
        "            # Collect all .jpeg images in the current subfolder\n",
        "            image_paths[subfolder_name] = [\n",
        "                os.path.join(subfolder.path, file)\n",
        "                for file in os.listdir(subfolder.path)\n",
        "                if file.lower().endswith('.jpeg')  # Only include .jpeg files\n",
        "            ]\n",
        "\n",
        "            # Debugging: Print the number of images found\n",
        "            if not image_paths[subfolder_name]:\n",
        "                print(f\"Warning: Subfolder {subfolder_name} contains no valid .jpeg files.\")\n",
        "            else:\n",
        "                print(f\"Subfolder {subfolder_name} contains {len(image_paths[subfolder_name])} .jpeg files.\")\n",
        "\n",
        "    # Return the dictionary of image paths and class labels\n",
        "     return image_paths, class_labels\n",
        "\n",
        "    def load(self, set_name, folder_name):\n",
        "        \"\"\"\n",
        "        Loads images from the specified folder and its subfolders, and assigns unique class labels for each subfolder.\n",
        "        Additionally, augment the dataset with 90, 180, 270, and 360-degree rotated versions of each image.\n",
        "        \"\"\"\n",
        "        print(f'Loading dataset from: {self.data_path}')\n",
        "\n",
        "        x_first = []\n",
        "        x_second = []\n",
        "        y = []\n",
        "        names = []\n",
        "\n",
        "        # Load images from the given folder and get their class labels\n",
        "        image_paths, class_labels = self.load_images_from_subfolders(folder_name)\n",
        "        print(f\"Image paths: {image_paths}\")\n",
        "        print(f\"Class labels: {class_labels}\")\n",
        "        # Pair images within the same subfolder and assign the corresponding class label\n",
        "        for subfolder, paths in image_paths.items():\n",
        "            for i, img1 in enumerate(paths):\n",
        "                print(paths)\n",
        "                image1 = Image.open(img1).convert('L').resize((self.width, self.height))\n",
        "\n",
        "                for j, img2 in enumerate(paths[i+1:]):\n",
        "                    image2 = Image.open(img2).convert('L').resize((self.width, self.height))\n",
        "\n",
        "                    # Original pair\n",
        "                    x_first.append(self._open_image(img1))\n",
        "                    x_second.append(self._open_image(img2))\n",
        "                    y.append(class_labels[subfolder])\n",
        "\n",
        "                    # Augment the dataset with 90, 180, 270, and 360-degree rotations\n",
        "                    for angle in [90, 180, 270]:\n",
        "                        x_first.append(self._rotate_image(image1, angle))\n",
        "                        x_second.append(self._rotate_image(image2, angle))\n",
        "                        y.append(class_labels[subfolder])\n",
        "\n",
        "        print(f'Done loading dataset: {len(x_first)} pairs loaded.')\n",
        "\n",
        "        # Save the augmented dataset as a pickle file\n",
        "        with open(self.output_path, 'wb') as f:\n",
        "            pickle.dump([[x_first, x_second], y, names], f)\n",
        "        print(f'Dataset saved to {self.output_path}')\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "class SiameseNetwork:\n",
        "    def __init__(self, seed, width, height, cells, num_classes, loss, metrics, optimizer, dropout_rate):\n",
        "        K.clear_session()\n",
        "        self.load_file = None\n",
        "        self.seed = seed\n",
        "        self.num_classes = num_classes  # Number of output classes\n",
        "        self.initialize_seed()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Define the input shapes\n",
        "        input_shape = (width, height, cells)\n",
        "        left_input = Input(input_shape)\n",
        "        right_input = Input(input_shape)\n",
        "\n",
        "        # Create the architecture for the Siamese network\n",
        "        model = self._get_architecture(input_shape)\n",
        "        encoded_l = model(left_input)\n",
        "        encoded_r = model(right_input)\n",
        "\n",
        "        # Compute the absolute difference between the encoded features\n",
        "        L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
        "        L1_siamese_dist = L1_layer([encoded_l, encoded_r])\n",
        "        L1_siamese_dist = Dropout(dropout_rate)(L1_siamese_dist)\n",
        "\n",
        "        # Output layer for multi-class classification (softmax for num_classes)\n",
        "        prediction = Dense(self.num_classes, activation='softmax', bias_initializer=self.initialize_bias)(L1_siamese_dist)\n",
        "        siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
        "        self.siamese_net = siamese_net\n",
        "        self.siamese_net.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def initialize_seed(self):\n",
        "        # Set random seed for reproducibility\n",
        "        os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        tf.random.set_seed(self.seed)\n",
        "\n",
        "    def initialize_weights(self, shape, dtype=None):\n",
        "        # Initialize weights for layers using He initializer for better generalization\n",
        "        return HeNormal()(shape)\n",
        "\n",
        "    def initialize_bias(self, shape, dtype=None):\n",
        "        # Initialize biases for layers\n",
        "        return K.random_normal(shape, mean=0.5, stddev=0.01, dtype=dtype, seed=self.seed)\n",
        "\n",
        "    def _get_architecture(self, input_shape):\n",
        "        # Define a simpler CNN architecture used by both \"arms\" of the Siamese network\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(10, 10), input_shape=input_shape, kernel_initializer=self.initialize_weights,\n",
        "                         kernel_regularizer=l2(5e-4), name='Conv1'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(Conv2D(64, kernel_size=(8, 8), kernel_initializer=self.initialize_weights,\n",
        "                         bias_initializer=self.initialize_bias, kernel_regularizer=l2(5e-4), name='Conv2'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "        model.add(Conv2D(128, kernel_size=(6, 6), kernel_initializer=self.initialize_weights,\n",
        "                         bias_initializer=self.initialize_bias, kernel_regularizer=l2(5e-4), name='Conv3'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1024, activation='sigmoid', kernel_initializer=self.initialize_weights,\n",
        "                        kernel_regularizer=l2(5e-3), bias_initializer=self.initialize_bias))\n",
        "        return model\n",
        "\n",
        "    def fit(self, weights_file, train_path, validation_size, batch_size, epochs, early_stopping, patience, min_delta):\n",
        "        # Load training data\n",
        "        with open(train_path, 'rb') as f:\n",
        "            x_train, y_train, names = pickle.load(f)\n",
        "\n",
        "        # Split into train/validation sets\n",
        "        x_train_0, x_val_0, y_train_0, y_val_0 = train_test_split(x_train[0], y_train, test_size=validation_size, random_state=self.seed)\n",
        "        x_train_1, x_val_1, y_train_1, y_val_1 = train_test_split(x_train[1], y_train, test_size=validation_size, random_state=self.seed)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        x_train_0 = np.array(x_train_0, dtype='float64')\n",
        "        x_val_0 = np.array(x_val_0, dtype='float64')\n",
        "        x_train_1 = np.array(x_train_1, dtype='float64')\n",
        "        x_val_1 = np.array(x_val_1, dtype='float64')\n",
        "\n",
        "        x_train = [x_train_0, x_train_1]\n",
        "        x_val = [x_val_0, x_val_1]\n",
        "\n",
        "        # Convert the class labels to one-hot encoded format (since there are multiple classes)\n",
        "        y_train_one_hot = to_categorical(y_train_0, num_classes=self.num_classes)\n",
        "        y_val_one_hot = to_categorical(y_val_0, num_classes=self.num_classes)\n",
        "\n",
        "        # Data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "\n",
        "        # Manually augment both left and right images\n",
        "        augmented_x_train_0 = []\n",
        "        augmented_x_train_1 = []\n",
        "        augmented_y_train = []\n",
        "\n",
        "        for x_0, x_1, y in zip(x_train_0, x_train_1, y_train_one_hot):\n",
        "            augmented_x_train_0.append(datagen.random_transform(x_0))\n",
        "            augmented_x_train_1.append(datagen.random_transform(x_1))\n",
        "            augmented_y_train.append(y)\n",
        "\n",
        "        augmented_x_train_0 = np.array(augmented_x_train_0)\n",
        "        augmented_x_train_1 = np.array(augmented_x_train_1)\n",
        "        augmented_y_train = np.array(augmented_y_train)\n",
        "\n",
        "        # Fit the model\n",
        "        print('Beginning to fit the model')\n",
        "        callback = []\n",
        "        if early_stopping:\n",
        "            es = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, mode='auto', verbose=1)\n",
        "            callback.append(es)\n",
        "\n",
        "        self.siamese_net.fit([augmented_x_train_0, augmented_x_train_1], augmented_y_train, batch_size=batch_size,\n",
        "                             validation_data=([x_val[0], x_val[1]], y_val_one_hot),\n",
        "                             epochs=epochs, callbacks=callback, verbose=1)\n",
        "\n",
        "        # Save weights\n",
        "        self.siamese_net.save_weights(weights_file)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        loss, accuracy = self.siamese_net.evaluate([x_val[0], x_val[1]], y_val_one_hot, batch_size=batch_size)\n",
        "        print(f'Loss on Validation set: {loss}')\n",
        "        print(f'Accuracy on Validation set: {accuracy}')\n",
        "\n",
        "    def evaluate(self, test_file, batch_size):\n",
        "        with open(test_file, 'rb') as f:\n",
        "            x_test, y_test, names = pickle.load(f)\n",
        "\n",
        "        y_test = np.array(y_test, dtype='float64')\n",
        "        x_test[0] = np.array(x_test[0], dtype='float64')\n",
        "        x_test[1] = np.array(x_test[1], dtype='float64')\n",
        "\n",
        "        # Convert y_test to one-hot encoding\n",
        "        y_test_one_hot = to_categorical(y_test, num_classes=self.num_classes)\n",
        "\n",
        "        # Perform evaluation\n",
        "        loss, accuracy = self.siamese_net.evaluate([x_test[0], x_test[1]], y_test_one_hot, batch_size=batch_size)\n",
        "\n",
        "        # Return the results\n",
        "        return loss, accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "evrzqVLUwnf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5irM7EIezWA",
        "outputId": "242dddb9-44df-479a-d2b0-fcc90ce6e414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pairs: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "# Environment settings\n",
        "IS_COLAB = (os.name == 'posix')  # Detect if running in Colab\n",
        "LOAD_DATA = True  # Ensure we load data\n",
        "IS_EXPERIMENT = False  # Toggle experiment mode\n",
        "\n",
        "train_name = 'train'\n",
        "test_name = 'test'\n",
        "WIDTH = HEIGHT = 105\n",
        "CEELS = 1\n",
        "loss_type = \"categorical_crossentropy\"  # Updated to categorical cross-entropy for multi-class\n",
        "validation_size = 0.2\n",
        "early_stopping = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = os.path.join('/content/drive/MyDrive/6th_sem_immune_dataset/SubClassification/Nuclear/Pleomorphic')  # Adapt to your Google Drive path\n",
        "else:\n",
        "    from data_loader import DataLoader\n",
        "    from siamese_network import SiameseNetwork\n",
        "    data_path = os.path.join('path_to_your_local_dataset')\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Function to determine the number of classes based on subfolders\n",
        "def get_num_classes(data_path):\n",
        "    subfolders = [f.name for f in os.scandir(data_path) if f.is_dir()]\n",
        "    return len(subfolders)\n",
        "\n",
        "def run_combination(l, bs, ep, pat, md, seed, train_path, test_path, num_classes):\n",
        "    model_save_type = 'h5'\n",
        "    initialize_seed(seed)\n",
        "    parameters_name = f'seed_{seed}_lr_{l}_bs_{bs}_ep_{ep}_val_{validation_size}_es_{early_stopping}_pa_{pat}_md_{md}'\n",
        "    print(f'Running combination with {parameters_name}')\n",
        "\n",
        "    # Ensure the weights directory exists\n",
        "    weights_dir = os.path.join(data_path, 'weights')\n",
        "    if not os.path.exists(weights_dir):\n",
        "        os.makedirs(weights_dir)\n",
        "\n",
        "    load_weights_path = os.path.join(weights_dir, f'weights_{parameters_name}.weights.h5')\n",
        "\n",
        "    siamese = SiameseNetwork(seed=seed, width=WIDTH, height=HEIGHT, cells=CEELS, num_classes=num_classes,\n",
        "                             loss=loss_type, metrics=['accuracy'], optimizer=Adam(learning_rate=l), dropout_rate=0.4)\n",
        "\n",
        "    print(load_weights_path, train_path, validation_size, bs,ep,early_stopping,pat,md)\n",
        "    siamese.fit(weights_file=load_weights_path, train_path=train_path, validation_size=validation_size,\n",
        "                batch_size=bs, epochs=ep, early_stopping=early_stopping, patience=pat, min_delta=md)\n",
        "\n",
        "    # Remove the 'analyze' argument from the evaluate method\n",
        "    loss, accuracy = siamese.evaluate(test_file=test_path, batch_size=bs)\n",
        "\n",
        "    print(f'Loss on Testing set: {loss}')\n",
        "    print(f'Accuracy on Testing set: {accuracy}')\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "def run():\n",
        "    data_set_save_type = 'pickle'\n",
        "    train_path = os.path.join(data_path, f'{train_name}.{data_set_save_type}')  # Path for train file\n",
        "    test_path = os.path.join(data_path, f'{test_name}.{data_set_save_type}')  # Path for test file\n",
        "\n",
        "    # Determine the number of classes based on the subfolders in the dataset\n",
        "    num_classes = get_num_classes(data_path)\n",
        "\n",
        "    if LOAD_DATA:\n",
        "        try:\n",
        "            loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=train_path)\n",
        "            loader.load(set_name=train_name, folder_name='')  # Pass empty string to load all subfolders\n",
        "            loader = DataLoader(width=WIDTH, height=HEIGHT, cells=CEELS, data_path=data_path, output_path=test_path)\n",
        "            loader.load(set_name=test_name, folder_name='')  # Pass empty string to load all subfolders\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {str(e)}\")\n",
        "            return\n",
        "\n",
        "    result_path = os.path.join(data_path, f'results.csv')  # Path for results\n",
        "    results = {'lr': [], 'batch_size': [], 'epochs': [], 'patience': [], 'min_delta': [], 'seed': [], 'loss': [], 'accuracy': []}\n",
        "\n",
        "    for l in lr:\n",
        "        for bs in batch_size:\n",
        "            for ep in epochs:\n",
        "                for pat in patience:\n",
        "                    for md in min_delta:\n",
        "                        for seed in seeds:\n",
        "                            loss, accuracy = run_combination(l=l, bs=bs, ep=ep, pat=pat, md=md, seed=seed,\n",
        "                                                             train_path=train_path, test_path=test_path, num_classes=num_classes)\n",
        "                            results['lr'].append(l)\n",
        "                            results['batch_size'].append(bs)\n",
        "                            results['epochs'].append(ep)\n",
        "                            results['patience'].append(pat)\n",
        "                            results['min_delta'].append(md)\n",
        "                            results['seed'].append(seed)\n",
        "                            results['loss'].append(loss)\n",
        "                            results['accuracy'].append(accuracy)\n",
        "\n",
        "    df_results = pd.DataFrame.from_dict(results)\n",
        "    df_results.to_csv(result_path)\n",
        "\n",
        "\n",
        "def initialize_seed(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if IS_EXPERIMENT:\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [32]\n",
        "        epochs = [10]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "    else:\n",
        "        seeds = [0]\n",
        "        lr = [0.00005]\n",
        "        batch_size = [32]\n",
        "        epochs = [10]\n",
        "        patience = [5]\n",
        "        min_delta = [0.1]\n",
        "\n",
        "    print(os.name)\n",
        "    start_time = time.time()\n",
        "    print('Starting the experiments')\n",
        "    run()\n",
        "    print(f'Total Running Time: {time.time() - start_time}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO_7wcqX2Ykz",
        "outputId": "f38f5422-cbd4-42ad-a5d0-efda8433429b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "posix\n",
            "Starting the experiments\n",
            "Loading dataset from: /content/drive/MyDrive/6th_sem_immune_dataset/SubClassification/Nuclear/Pleomorphic\n",
            "Error loading data: [Errno 2] No such file or directory: ''\n",
            "Total Running Time: 0.002100229263305664\n"
          ]
        }
      ]
    }
  ]
}